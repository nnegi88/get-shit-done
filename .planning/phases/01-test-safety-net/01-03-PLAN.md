---
phase: 01-test-safety-net
plan: 03
type: execute
wave: 2
depends_on: ["01-02"]
files_modified:
  - get-shit-done/bin/gsd-tools.test.js
autonomous: true

must_haves:
  truths:
    - "Every frontmatter, template, and verify subcommand has at least one characterization test"
    - "Every init subcommand has at least one characterization test capturing its output contract"
    - "Phase numbering edge cases (double-digit phases, 1.9->1.10 decimal transition, lexicographic vs numeric sort) are tested"
    - "Every tested command has at least one error recovery test (invalid input, missing files, or corrupt state)"
  artifacts:
    - path: "get-shit-done/bin/gsd-tools.test.js"
      provides: "Complete characterization test suite for all 70 command paths"
      min_lines: 3800
  key_links:
    - from: "get-shit-done/bin/gsd-tools.test.js"
      to: "get-shit-done/bin/gsd-tools.js"
      via: "execSync subprocess invocation"
      pattern: "runGsdTools\\("
---

<objective>
Complete gsd-tools.js characterization test coverage with Tier 3 (frontmatter/template/verify), Tier 4 (init commands), Tier 5 (error recovery paths), and TEST-06 (phase numbering edge cases).

Purpose: After Plan 02 covers standalone and state commands, this plan completes the remaining command coverage. Tier 3 and Tier 4 commands touch more complex subsystems (frontmatter parsing, template rendering, verification checks, init workflows) and benefit from the test patterns established in Plan 02. Error recovery paths (Tier 5) ensure no command silently swallows bad input.

Output: ~80-110 new tests completing full characterization coverage of all 70 command paths.
</objective>

<execution_context>
@/Users/naveennegi/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naveennegi/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@get-shit-done/bin/gsd-tools.js
@get-shit-done/bin/gsd-tools.test.js
@.planning/phases/01-test-safety-net/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add characterization tests for Tier 3 frontmatter, template, and verify subcommands</name>
  <files>get-shit-done/bin/gsd-tools.test.js</files>
  <action>
  Add describe blocks for each untested Tier 3 command. These commands work with frontmatter YAML, templates, and file verification.

  **Frontmatter subcommands:**

  `describe('frontmatter get command', ...)` (3 tests):
  1. Extracts specific field from valid frontmatter
  2. Returns all frontmatter when no field specified
  3. Handles missing file gracefully

  `describe('frontmatter set command', ...)` (3 tests):
  1. Sets field value in existing frontmatter
  2. Creates frontmatter if file has none
  3. Preserves body content after frontmatter

  `describe('frontmatter merge command', ...)` (3 tests):
  1. Merges JSON fields into existing frontmatter
  2. Handles empty/new frontmatter
  3. Deep-merges nested fields (e.g., must_haves)

  `describe('frontmatter validate command', ...)` (3 tests):
  1. Valid plan frontmatter passes validation (provide --schema plan)
  2. Valid summary frontmatter passes validation (provide --schema summary)
  3. Missing required fields returns validation errors

  **Template subcommands:**

  `describe('template select command', ...)` (2 tests):
  1. Simple plan type returns correct template identifier
  2. TDD plan type returns correct template

  `describe('template fill summary command', ...)` (2 tests):
  1. Fills summary template with provided field values
  2. Handles missing fields gracefully (defaults or errors)

  `describe('template fill plan command', ...)` (2 tests):
  1. Execute type plan fills correctly
  2. TDD type plan fills correctly

  `describe('template fill verification command', ...)` (2 tests):
  1. Fills verification template with fields
  2. Handles missing fields

  **Verify subcommands:**

  `describe('verify plan-structure command', ...)` (3 tests):
  1. Valid plan with tasks passes
  2. Plan missing sections returns errors
  3. Plan with no tasks returns errors

  `describe('verify phase-completeness command', ...)` (3 tests):
  1. Complete phase (all summaries exist) passes
  2. Incomplete phase (missing summaries) returns incomplete items
  3. Empty phase directory

  `describe('verify references command', ...)` (3 tests):
  1. Valid @-references all resolve
  2. Broken references detected
  3. File with no references passes cleanly

  `describe('verify commits command', ...)` (2 tests):
  1. Valid commit hash returns success (requires git init in tmpDir)
  2. Invalid commit hash returns failure

  `describe('verify artifacts command', ...)` (2 tests):
  1. All listed artifacts exist -> pass
  2. Some artifacts missing -> lists missing files

  `describe('verify key-links command', ...)` (2 tests):
  1. Valid key links pass verification
  2. Broken key links detected

  For git-dependent commands (verify commits): Initialize git repo in tmpDir with `execSync('git init && git config user.email "test@test.com" && git config user.name "Test"', { cwd: tmpDir })` and create an initial commit.

  Total: ~35-40 new tests.
  </action>
  <verify>Run `node --test get-shit-done/bin/gsd-tools.test.js --test-name-pattern "frontmatter|template|verify"` and confirm all new tests pass.</verify>
  <done>All Tier 3 frontmatter, template, and verify subcommands have characterization tests with 35+ new tests passing.</done>
</task>

<task type="auto">
  <name>Task 2: Add characterization tests for Tier 4 init commands and TEST-06 phase numbering edge cases</name>
  <files>get-shit-done/bin/gsd-tools.test.js</files>
  <action>
  Add describe blocks for untested init subcommands and extend the existing `phase next-decimal` describe block with edge cases.

  **Init subcommands** (add tests only for commands not already covered by existing describe blocks -- check the existing `init commands with --include flag` block):

  `describe('init new-project command', ...)` (2 tests):
  1. Fresh project creates .planning structure
  2. Existing .planning directory handled (idempotent or error)

  `describe('init new-milestone command', ...)` (2 tests):
  1. Creates milestone structure with version
  2. Handles project without milestones directory

  `describe('init quick command', ...)` (2 tests):
  1. Quick init with description creates minimal state
  2. Quick init creates config.json with defaults

  `describe('init resume command', ...)` (2 tests):
  1. With session data returns resume context
  2. Without session data returns empty/default

  `describe('init verify-work command', ...)` (2 tests):
  1. Valid phase returns verification init context
  2. Invalid phase handled gracefully

  `describe('init phase-op command', ...)` (2 tests):
  1. Existing phase returns phase operation context
  2. Non-existent phase returns appropriate error/empty

  `describe('init todos command', ...)` (2 tests):
  1. With todos returns todo init context
  2. Without todos returns empty context

  `describe('init milestone-op command', ...)` (2 tests):
  1. Standard scenario returns milestone operation context
  2. No milestones handled gracefully

  `describe('init map-codebase command', ...)` (1 test):
  1. Returns codebase mapping init context

  For existing init commands that are partially tested (init execute-phase, init plan-phase, init progress), add 1-2 additional edge case tests each.

  **Phase numbering edge cases (TEST-06)** -- extend the existing `describe('phase next-decimal command', ...)` block:

  1. Double-digit base phase (e.g., phase 10 -> next decimal 10.1)
  2. Decimal transition from 1.9 to 1.10 (not 1.10 sorted before 1.9 lexicographically)
  3. Many decimal phases sort numerically (1.1, 1.2, 1.9, 1.10, 1.11 -> next is 1.12)
  4. Base phase with no existing decimals -> returns X.1
  5. Non-existent base phase handled gracefully

  NOTE: Check if some of these phase-numbering tests already exist in the existing describe block. Only add tests that are genuinely missing. The research identified these specific edge cases as needed.

  Total: ~25-30 new tests.
  </action>
  <verify>Run `node --test get-shit-done/bin/gsd-tools.test.js --test-name-pattern "init |phase next-decimal"` and confirm all new tests pass. Then run the full suite to confirm no regressions.</verify>
  <done>All init subcommands have characterization tests. Phase numbering edge cases (double-digit, 1.9->1.10, numeric vs lexicographic sort) are tested per TEST-06.</done>
</task>

<task type="auto">
  <name>Task 3: Add error recovery tests for all commands (TEST-05)</name>
  <files>get-shit-done/bin/gsd-tools.test.js</files>
  <action>
  Add error recovery tests distributed across existing and new describe blocks. The goal is that EVERY command has at least one test verifying graceful handling of bad input. This covers TEST-05 (error recovery paths).

  Strategy: For each describe block (both existing and newly added), add 1-2 error tests if they don't already have one. Error tests verify that the command:
  - Returns a non-zero exit code OR a JSON response with an error/failure indicator
  - Does NOT crash with an unhandled exception
  - Produces a meaningful error message (not empty stderr)

  **Error scenarios to test by category:**

  1. **Missing .planning directory**: Run commands against a tmpDir without `.planning/` -> verify graceful error
  2. **Invalid arguments**: Pass wrong argument types or missing required args -> verify usage error
  3. **Corrupt state files**: Create STATE.md with invalid content -> verify commands don't crash
  4. **Missing referenced files**: frontmatter get on non-existent file, verify artifacts with missing files
  5. **Empty input where content expected**: Empty ROADMAP.md, empty phase directories, empty frontmatter

  Add tests grouped logically. You can add an `describe('error recovery paths', ...)` meta-block OR add individual error tests to each existing describe block -- choose whichever produces cleaner, more maintainable tests.

  Key commands to prioritize for error testing:
  - `state load` with corrupt config.json (malformed JSON)
  - `roadmap get-phase` with empty ROADMAP.md
  - `frontmatter get` with binary/non-text file
  - `phase-plan-index` with corrupt PLAN.md files
  - `summary-extract` with missing summary files
  - `init execute-phase` with non-existent phase
  - `validate consistency` with partially corrupt state
  - `commit` with no git repo (already covered if exists -- check)
  - `websearch` without API key (test "no API key" path only)

  Total: ~20-30 error recovery tests.
  </action>
  <verify>Run the full test suite `node --test get-shit-done/bin/gsd-tools.test.js` and confirm ALL tests pass (existing + Plan 02 + Plan 03 tasks 1-2 + error recovery). Verify total test count is 190+ tests.</verify>
  <done>Every tested command has at least one error recovery test. No command crashes on invalid input, missing files, or corrupt state. Total test count in gsd-tools.test.js is 190+.</done>
</task>

</tasks>

<verification>
1. `node --test get-shit-done/bin/gsd-tools.test.js` passes with 190+ tests
2. Every frontmatter/template/verify subcommand has characterization tests
3. Every init subcommand has characterization tests
4. Phase numbering edge cases (double-digit, 1.9->1.10) tested per TEST-06
5. Every command has at least one error recovery test per TEST-05
6. No existing tests broken
</verification>

<success_criteria>
- All 70 command paths in gsd-tools.js have at least one characterization test
- Phase numbering edge cases (TEST-06) tested including 1.9->1.10 transition
- Error recovery paths (TEST-05) covered for all commands
- Full test suite passes with 190+ tests
- No command crashes silently on bad input
</success_criteria>

<output>
After completion, create `.planning/phases/01-test-safety-net/01-03-SUMMARY.md`
</output>
